{"version":3,"sources":["index.js"],"names":[],"mappings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file":"index.js","sourcesContent":["\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.TokenData = void 0;\nexports.parse = parse;\nexports.compile = compile;\nexports.match = match;\nexports.pathToRegexp = pathToRegexp;\nexports.stringify = stringify;\nconst DEFAULT_DELIMITER = \"/\";\nconst NOOP_VALUE = (value) => value;\nconst ID_START = /^[$_\\p{ID_Start}]$/u;\nconst ID_CONTINUE = /^[$\\u200c\\u200d\\p{ID_Continue}]$/u;\nconst DEBUG_URL = \"https://git.new/pathToRegexpError\";\nconst SIMPLE_TOKENS = {\n    // Groups.\n    \"{\": \"{\",\n    \"}\": \"}\",\n    // Reserved.\n    \"(\": \"(\",\n    \")\": \")\",\n    \"[\": \"[\",\n    \"]\": \"]\",\n    \"+\": \"+\",\n    \"?\": \"?\",\n    \"!\": \"!\",\n};\n/**\n * Escape text for stringify to path.\n */\nfunction escapeText(str) {\n    return str.replace(/[{}()\\[\\]+?!:*]/g, \"\\\\$&\");\n}\n/**\n * Escape a regular expression string.\n */\nfunction escape(str) {\n    return str.replace(/[.+*?^${}()[\\]|/\\\\]/g, \"\\\\$&\");\n}\n/**\n * Tokenize input string.\n */\nfunction* lexer(str) {\n    const chars = [...str];\n    let i = 0;\n    function name() {\n        let value = \"\";\n        if (ID_START.test(chars[++i])) {\n            value += chars[i];\n            while (ID_CONTINUE.test(chars[++i])) {\n                value += chars[i];\n            }\n        }\n        else if (chars[i] === '\"') {\n            let pos = i;\n            while (i < chars.length) {\n                if (chars[++i] === '\"') {\n                    i++;\n                    pos = 0;\n                    break;\n                }\n                if (chars[i] === \"\\\\\") {\n                    value += chars[++i];\n                }\n                else {\n                    value += chars[i];\n                }\n            }\n            if (pos) {\n                throw new TypeError(`Unterminated quote at ${pos}: ${DEBUG_URL}`);\n            }\n        }\n        if (!value) {\n            throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`);\n        }\n        return value;\n    }\n    while (i < chars.length) {\n        const value = chars[i];\n        const type = SIMPLE_TOKENS[value];\n        if (type) {\n            yield { type, index: i++, value };\n        }\n        else if (value === \"\\\\\") {\n            yield { type: \"ESCAPED\", index: i++, value: chars[i++] };\n        }\n        else if (value === \":\") {\n            const value = name();\n            yield { type: \"PARAM\", index: i, value };\n        }\n        else if (value === \"*\") {\n            const value = name();\n            yield { type: \"WILDCARD\", index: i, value };\n        }\n        else {\n            yield { type: \"CHAR\", index: i, value: chars[i++] };\n        }\n    }\n    return { type: \"END\", index: i, value: \"\" };\n}\nclass Iter {\n    constructor(tokens) {\n        this.tokens = tokens;\n    }\n    peek() {\n        if (!this._peek) {\n            const next = this.tokens.next();\n            this._peek = next.value;\n        }\n        return this._peek;\n    }\n    tryConsume(type) {\n        const token = this.peek();\n        if (token.type !== type)\n            return;\n        this._peek = undefined; // Reset after consumed.\n        return token.value;\n    }\n    consume(type) {\n        const value = this.tryConsume(type);\n        if (value !== undefined)\n            return value;\n        const { type: nextType, index } = this.peek();\n        throw new TypeError(`Unexpected ${nextType} at ${index}, expected ${type}: ${DEBUG_URL}`);\n    }\n    text() {\n        let result = \"\";\n        let value;\n        while ((value = this.tryConsume(\"CHAR\") || this.tryConsume(\"ESCAPED\"))) {\n            result += value;\n        }\n        return result;\n    }\n}\n/**\n * Tokenized path instance.\n */\nclass TokenData {\n    constructor(tokens) {\n        this.tokens = tokens;\n    }\n}\nexports.TokenData = TokenData;\n/**\n * Parse a string for the raw tokens.\n */\nfunction parse(str, options = {}) {\n    const { encodePath = NOOP_VALUE } = options;\n    const it = new Iter(lexer(str));\n    function consume(endType) {\n        const tokens = [];\n        while (true) {\n            const path = it.text();\n            if (path)\n                tokens.push({ type: \"text\", value: encodePath(path) });\n            const param = it.tryConsume(\"PARAM\");\n            if (param) {\n                tokens.push({\n                    type: \"param\",\n                    name: param,\n                });\n                continue;\n            }\n            const wildcard = it.tryConsume(\"WILDCARD\");\n            if (wildcard) {\n                tokens.push({\n                    type: \"wildcard\",\n                    name: wildcard,\n                });\n                continue;\n            }\n            const open = it.tryConsume(\"{\");\n            if (open) {\n                tokens.push({\n                    type: \"group\",\n                    tokens: consume(\"}\"),\n                });\n                continue;\n            }\n            it.consume(endType);\n            return tokens;\n        }\n    }\n    const tokens = consume(\"END\");\n    return new TokenData(tokens);\n}\n/**\n * Compile a string to a template function for the path.\n */\nfunction compile(path, options = {}) {\n    const { encode = encodeURIComponent, delimiter = DEFAULT_DELIMITER } = options;\n    const data = path instanceof TokenData ? path : parse(path, options);\n    const fn = tokensToFunction(data.tokens, delimiter, encode);\n    return function path(data = {}) {\n        const [path, ...missing] = fn(data);\n        if (missing.length) {\n            throw new TypeError(`Missing parameters: ${missing.join(\", \")}`);\n        }\n        return path;\n    };\n}\nfunction tokensToFunction(tokens, delimiter, encode) {\n    const encoders = tokens.map((token) => tokenToFunction(token, delimiter, encode));\n    return (data) => {\n        const result = [\"\"];\n        for (const encoder of encoders) {\n            const [value, ...extras] = encoder(data);\n            result[0] += value;\n            result.push(...extras);\n        }\n        return result;\n    };\n}\n/**\n * Convert a single token into a path building function.\n */\nfunction tokenToFunction(token, delimiter, encode) {\n    if (token.type === \"text\")\n        return () => [token.value];\n    if (token.type === \"group\") {\n        const fn = tokensToFunction(token.tokens, delimiter, encode);\n        return (data) => {\n            const [value, ...missing] = fn(data);\n            if (!missing.length)\n                return [value];\n            return [\"\"];\n        };\n    }\n    const encodeValue = encode || NOOP_VALUE;\n    if (token.type === \"wildcard\" && encode !== false) {\n        return (data) => {\n            const value = data[token.name];\n            if (value == null)\n                return [\"\", token.name];\n            if (!Array.isArray(value) || value.length === 0) {\n                throw new TypeError(`Expected \"${token.name}\" to be a non-empty array`);\n            }\n            return [\n                value\n                    .map((value, index) => {\n                    if (typeof value !== \"string\") {\n                        throw new TypeError(`Expected \"${token.name}/${index}\" to be a string`);\n                    }\n                    return encodeValue(value);\n                })\n                    .join(delimiter),\n            ];\n        };\n    }\n    return (data) => {\n        const value = data[token.name];\n        if (value == null)\n            return [\"\", token.name];\n        if (typeof value !== \"string\") {\n            throw new TypeError(`Expected \"${token.name}\" to be a string`);\n        }\n        return [encodeValue(value)];\n    };\n}\n/**\n * Transform a path into a match function.\n */\nfunction match(path, options = {}) {\n    const { decode = decodeURIComponent, delimiter = DEFAULT_DELIMITER } = options;\n    const { regexp, keys } = pathToRegexp(path, options);\n    const decoders = keys.map((key) => {\n        if (decode === false)\n            return NOOP_VALUE;\n        if (key.type === \"param\")\n            return decode;\n        return (value) => value.split(delimiter).map(decode);\n    });\n    return function match(input) {\n        const m = regexp.exec(input);\n        if (!m)\n            return false;\n        const path = m[0];\n        const params = Object.create(null);\n        for (let i = 1; i < m.length; i++) {\n            if (m[i] === undefined)\n                continue;\n            const key = keys[i - 1];\n            const decoder = decoders[i - 1];\n            params[key.name] = decoder(m[i]);\n        }\n        return { path, params };\n    };\n}\nfunction pathToRegexp(path, options = {}) {\n    const { delimiter = DEFAULT_DELIMITER, end = true, sensitive = false, trailing = true, } = options;\n    const keys = [];\n    const sources = [];\n    const flags = sensitive ? \"\" : \"i\";\n    const paths = Array.isArray(path) ? path : [path];\n    const items = paths.map((path) => path instanceof TokenData ? path : parse(path, options));\n    for (const { tokens } of items) {\n        for (const seq of flatten(tokens, 0, [])) {\n            const regexp = sequenceToRegExp(seq, delimiter, keys);\n            sources.push(regexp);\n        }\n    }\n    let pattern = `^(?:${sources.join(\"|\")})`;\n    if (trailing)\n        pattern += `(?:${escape(delimiter)}$)?`;\n    pattern += end ? \"$\" : `(?=${escape(delimiter)}|$)`;\n    const regexp = new RegExp(pattern, flags);\n    return { regexp, keys };\n}\n/**\n * Generate a flat list of sequence tokens from the given tokens.\n */\nfunction* flatten(tokens, index, init) {\n    if (index === tokens.length) {\n        return yield init;\n    }\n    const token = tokens[index];\n    if (token.type === \"group\") {\n        const fork = init.slice();\n        for (const seq of flatten(token.tokens, 0, fork)) {\n            yield* flatten(tokens, index + 1, seq);\n        }\n    }\n    else {\n        init.push(token);\n    }\n    yield* flatten(tokens, index + 1, init);\n}\n/**\n * Transform a flat sequence of tokens into a regular expression.\n */\nfunction sequenceToRegExp(tokens, delimiter, keys) {\n    let result = \"\";\n    let backtrack = \"\";\n    let isSafeSegmentParam = true;\n    for (let i = 0; i < tokens.length; i++) {\n        const token = tokens[i];\n        if (token.type === \"text\") {\n            result += escape(token.value);\n            backtrack += token.value;\n            isSafeSegmentParam || (isSafeSegmentParam = token.value.includes(delimiter));\n            continue;\n        }\n        if (token.type === \"param\" || token.type === \"wildcard\") {\n            if (!isSafeSegmentParam && !backtrack) {\n                throw new TypeError(`Missing text after \"${token.name}\": ${DEBUG_URL}`);\n            }\n            if (token.type === \"param\") {\n                result += `(${negate(delimiter, isSafeSegmentParam ? \"\" : backtrack)}+)`;\n            }\n            else {\n                result += `([\\\\s\\\\S]+)`;\n            }\n            keys.push(token);\n            backtrack = \"\";\n            isSafeSegmentParam = false;\n            continue;\n        }\n    }\n    return result;\n}\nfunction negate(delimiter, backtrack) {\n    if (backtrack.length < 2) {\n        if (delimiter.length < 2)\n            return `[^${escape(delimiter + backtrack)}]`;\n        return `(?:(?!${escape(delimiter)})[^${escape(backtrack)}])`;\n    }\n    if (delimiter.length < 2) {\n        return `(?:(?!${escape(backtrack)})[^${escape(delimiter)}])`;\n    }\n    return `(?:(?!${escape(backtrack)}|${escape(delimiter)})[\\\\s\\\\S])`;\n}\n/**\n * Stringify token data into a path string.\n */\nfunction stringify(data) {\n    return data.tokens\n        .map(function stringifyToken(token, index, tokens) {\n        if (token.type === \"text\")\n            return escapeText(token.value);\n        if (token.type === \"group\") {\n            return `{${token.tokens.map(stringifyToken).join(\"\")}}`;\n        }\n        const isSafe = isNameSafe(token.name) && isNextNameSafe(tokens[index + 1]);\n        const key = isSafe ? token.name : JSON.stringify(token.name);\n        if (token.type === \"param\")\n            return `:${key}`;\n        if (token.type === \"wildcard\")\n            return `*${key}`;\n        throw new TypeError(`Unexpected token: ${token}`);\n    })\n        .join(\"\");\n}\nfunction isNameSafe(name) {\n    const [first, ...rest] = name;\n    if (!ID_START.test(first))\n        return false;\n    return rest.every((char) => ID_CONTINUE.test(char));\n}\nfunction isNextNameSafe(token) {\n    if ((token === null || token === void 0 ? void 0 : token.type) !== \"text\")\n        return true;\n    return !ID_CONTINUE.test(token.value[0]);\n}\n//# sourceMappingURL=index.js.map"]}